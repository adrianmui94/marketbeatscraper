{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "current-bailey",
   "metadata": {},
   "source": [
    "# Marketbeat Scraper\n",
    "\n",
    "Redone as a class object. For practice.\n",
    "\n",
    "Iterates through the stock analyst upgrades/downgrades on marketbeat.com by day and pulls the into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "placed-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert selenium chromedriver is up to date\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Web scraping libraries \n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "\n",
    "# BeautifulSoup \n",
    "import bs4 \n",
    "\n",
    "# Common libraries \n",
    "import os \n",
    "import re\n",
    "from pprint import pprint \n",
    "from tqdm import tqdm\n",
    "import datetime as dt \n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accessible-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketbeatScraper():\n",
    "    def __init__(self, initial_date = '01/01/2015', cwd = os.getcwd()):\n",
    "        \"\"\"\n",
    "        Creates a \n",
    "        \"\"\"\n",
    "        self.initial_date = initial_date\n",
    "        # Initialize webdriver\n",
    "        self.wb = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        url = 'https://www.marketbeat.com/ratings/'\n",
    "        self.wb.get(url)\n",
    "        self.handle_first_popup()\n",
    "        self.change_date(initial_date)\n",
    "        \n",
    "        # Make csv file if doesn't exist\n",
    "        if 'marketbeat_analyst_data.csv' not in os.listdir(cwd):\n",
    "            with open('marketbeat_analyst_data.csv', 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['date', 'ticker', 'company', 'action', 'brokerage',\n",
    "                                 'pt1', 'pt2', 'rating1', 'rating2', 'impact'])\n",
    "                \n",
    "        # Check scraped dates so we don't have to scrape them again.\n",
    "        with open('marketbeat_analyst_data.csv', 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            self.completed_dates = set([row[0] for row in list(reader)[1:]])\n",
    "    \n",
    "    def handle_first_popup(self):\n",
    "        '''\n",
    "        Done to clear the first popup that happens.\n",
    "        '''\n",
    "        if self.wb.find_element_by_id('optinform-modal'):\n",
    "            self.wb.find_element_by_class_name('x').click()\n",
    "\n",
    "    def handle_other_popup(self):\n",
    "        '''\n",
    "        Other popups can occur during the scraping process.\n",
    "        '''\n",
    "        try:\n",
    "            if self.wb.find_element_by_xpath('//*[@id=\"optinform-modal\"]/div/span'):\n",
    "                self.wb.find_element_by_xpath('//*[@id=\"optinform-modal\"]/div/span').click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def change_date(self, date):\n",
    "        '''\n",
    "        Changes the date on the analyst website for Marketbeat.\n",
    "        This is necessary because using clearing the datefield with clear() will reset to the previous date \n",
    "        since there is no cursor inside the field.\n",
    "        --------------------------------------------------------------------------------------------------\n",
    "        INPUT:\n",
    "        date - the date in either datetime or string format\n",
    "\n",
    "\n",
    "        OUTPUT:\n",
    "        None\n",
    "        '''\n",
    "        if type(date) == type(dt.datetime.today().date()):\n",
    "            date = date.strftime('%m/%d/%Y')\n",
    "        # Submitting date information \n",
    "\n",
    "        # Click on the date field\n",
    "        date_field = self.wb.find_element_by_name('ctl00$cphPrimaryContent$txtStartDate')\n",
    "        date_field.click()\n",
    "\n",
    "        # Backspace the current date \n",
    "        for _ in range(11):\n",
    "            webdriver.ActionChains(self.wb).send_keys(Keys.BACKSPACE).perform()\n",
    "\n",
    "        # Send a new date\n",
    "        date_field.send_keys(date)\n",
    "        webdriver.ActionChains(self.wb).send_keys(Keys.ENTER).perform()\n",
    "        \n",
    "    def scrape_values(self, html, page_date_str):\n",
    "        '''\n",
    "        Pulls the tabular analyst data from the current marketbeat analyst page using BeautifulSoup4\n",
    "        --------------------------------------------------------------------------------------------------\n",
    "        INPUT:\n",
    "        html - Can be from selenium page source or via requests.\n",
    "\n",
    "        OUTPUT:\n",
    "        scraped - 2D array of scraped values.\n",
    "        '''\n",
    "        scraped = []\n",
    "        soup = bs4.BeautifulSoup(html)\n",
    "\n",
    "        # The rows of the table are enclosed within the 'tr' tag.\n",
    "        table_rows =  soup.find_all('tr')\n",
    "\n",
    "        # Iterate through the rows of the table, ignoring the header\n",
    "        for row in table_rows[1:]:\n",
    "\n",
    "            # Grab the ticker & company name\n",
    "            try:\n",
    "                ticker = row.select('.ticker-area')[0].text.strip().upper()\n",
    "                company_name = row.select('.title-area')[0].text.strip().upper()\n",
    "\n",
    "            # One row in this table is always an ad (?)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            # The remaining columns do not have unique names and are enclosed in the 'td' tag\n",
    "            # The first td tag includes the things scraped above, so we ignore it\n",
    "            action_description = row.find_all('td')[1:]\n",
    "\n",
    "            # Clean the list of their tags\n",
    "            cleaned = list(map(lambda x: x.text.strip(), action_description))[:-1] # Last item is useless\n",
    "            del cleaned[2] # Current price is useless\n",
    "\n",
    "            # Extract 0, 1, or 2 price targets\n",
    "            clean_targets = self.handle_price_targets(cleaned[2]) \n",
    "\n",
    "            # Extract 1 or 2 price targets\n",
    "            clean_ratings = self.handle_ratings(cleaned[-2])\n",
    "\n",
    "            # Formatted row for appending to master rows\n",
    "            scraped_row = [page_date_str, ticker, company_name] + cleaned[:2] + clean_targets + clean_ratings + [cleaned[-1]]\n",
    "            scraped.append(scraped_row)\n",
    "        \n",
    "        return scraped\n",
    "    \n",
    "    def handle_price_targets(self, price_target):\n",
    "        '''\n",
    "        Marketbeat price target column can be empty, have one number, or two numbers.\n",
    "        Use this function to return all the numbers in separate columns.\n",
    "        --------------------------------------------------------------------------------------------------\n",
    "        INPUT:\n",
    "        price_target - string, targets in the format $\\d\\d.\\d\\d. Can include a '➝' character indicating a price change.\n",
    "\n",
    "        OUTPUT:\n",
    "        clean_targets - list of 2 floats or Nones. The previous price target and the new one.\n",
    "        '''\n",
    "        # Remove impurities \n",
    "        price_target = price_target.replace('$', '').replace(',', '')\n",
    "        price_target = price_target.replace('(', '').replace(')', '')\n",
    "        price_target = price_target.replace('0.0%', '')\n",
    "        price_target = re.split('[+-]', price_target)[0]\n",
    "\n",
    "        # Case 1: The field is blank.\n",
    "        if price_target == '':\n",
    "            clean_targets = [None, None]\n",
    "\n",
    "        # Case 2: The price target has changed and is indicated by an arrow\n",
    "        elif '➝' in price_target:\n",
    "            clean_targets = price_target.split('➝')\n",
    "            clean_targets = [float(ct.strip()) for ct in clean_targets]\n",
    "\n",
    "        else:\n",
    "            clean_targets = [None, float(price_target.strip())]\n",
    "\n",
    "        return clean_targets\n",
    "    \n",
    "    def handle_ratings(self, ratings):\n",
    "        '''\n",
    "        Marketbeat upgrades/downgrades show a change of rating (e.g. Neutral ➝ Outperform) or just have a single\n",
    "        rating present (Neutral)\n",
    "        Use this function to grab one or both actions.\n",
    "        --------------------------------------------------------------------------------------------------\n",
    "        INPUT:\n",
    "        ratings - string, either has 0, 1, or 2 ratings\n",
    "\n",
    "        OUTPUT:\n",
    "        clean_ratings - list containing two different ratings, or one None and one rating.\n",
    "        '''\n",
    "        ratings = ratings.split('➝')\n",
    "        if len(ratings) == 2:\n",
    "            ratings = [rating.strip().upper() for rating in ratings]\n",
    "        else:\n",
    "            ratings = [None, ratings[0].upper()]\n",
    "            if ratings[1] == '': ratings[1] = None\n",
    "\n",
    "        return ratings\n",
    "        \n",
    "    def scrape_marketbeat(self):\n",
    "        '''\n",
    "        The full scraping program\n",
    "        '''\n",
    "\n",
    "        page_date = dt.datetime.strptime(self.initial_date, '%m/%d/%Y')\n",
    "\n",
    "        while page_date <= (dt.datetime.today() + dt.timedelta(days=1)):\n",
    "            # Handle weekends\n",
    "            if page_date.strftime('%A') in ['Saturday', 'Sunday']:\n",
    "                page_date += dt.timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            # Use try/except to deal with selenium issues \n",
    "            # i.e. page loading too fast and elements not loading\n",
    "            try:\n",
    "                page_date_str = page_date.strftime('%m/%d/%Y')\n",
    "                if page_date_str in self.completed_dates:\n",
    "                    page_date += dt.timedelta(days=1)\n",
    "                    continue\n",
    "            except:\n",
    "            # Close other popup if exists\n",
    "                self.handle_other_popup()\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "\n",
    "            self.change_date(page_date_str)\n",
    "            time.sleep(1.5)\n",
    "            html = self.wb.page_source\n",
    "            scraped = self.scrape_values(html, page_date_str)\n",
    "\n",
    "            # Create a file to save your analyst ratings if one doesn't exist\n",
    "            if scraped != []:\n",
    "                with open('marketbeat_analyst_data.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                    writer = csv.writer(f, delimiter = ',')\n",
    "                    for row in scraped:\n",
    "                        writer.writerow(row)\n",
    "\n",
    "            # Increment by 1 day after completion\n",
    "            page_date += dt.timedelta(days=1)\n",
    "\n",
    "        print('Finished.')\n",
    "        self.wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "solid-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 93.0.4577\n",
      "[WDM] - Get LATEST driver version for 93.0.4577\n",
      "[WDM] - Driver [C:\\Users\\Mui\\.wdm\\drivers\\chromedriver\\win32\\93.0.4577.15\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mbs = MarketbeatScraper()\n",
    "    mbs.scrape_marketbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-banana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
